#!/bin/bash
#SBATCH --time=1:00:00 # walltime, abbreviated by -t
#SBATCH --nodes=2      # number of cluster nodes, abbreviated by -N
#SBATCH -o slurm-%j.out-%N # name of the stdout, using the job number (%j) and the first node (%N)
#SBATCH -e slurm-%j.err-%N # name of the stderr, using job and first node values
#SBATCH --ntasks=9    # number of MPI tasks, abbreviated by -n
# additional information for allocated clusters
#SBATCH --account=hpc_hpcadmin6     # account - abbreviated by -A
#SBATCH --partition=checkpt  # partition, abbreviated by -p

pwd

echo $SLURM_NTASKS
echo $PBS_NODEFILE
mpirun -np $SLURM_NTASKS hostname

#
# set data and working directories
#setenv WORKDIR $HOME/mydata
#setenv SCRDIR /scratch/kingspeak/serial/UNID/myscratch
#mkdir -p $SCRDIR
#cp -r $WORKDIR/* $SCRDIR
#cd $SCRDIR
##
## load appropriate modules, in this case Intel compilers, MPICH2
#module load intel mpich2
## for MPICH2 over Ethernet, set communication method to TCP
## see below for network interface selection options for different MPI distributions
#setenv MPICH_NEMESIS_NETMOD tcp
## run the program
## see below for ways to do this for different MPI distributions
#mpirun -np $SLURM_NTASKS my_mpi_program > my_program.out

